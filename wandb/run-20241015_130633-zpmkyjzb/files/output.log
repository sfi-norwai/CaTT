[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.




  4%|█████▌                                                                                                                                     | 4/100 [00:47<19:19, 12.08s/it]/Users/shamba/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.
  return fit_method(estimator, *args, **kwargs)




  9%|████████████▌                                                                                                                              | 9/100 [01:48<17:50, 11.76s/it]/Users/shamba/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.
  return fit_method(estimator, *args, **kwargs)




























 38%|████████████████████████████████████████████████████▍                                                                                     | 38/100 [07:56<12:57, 12.54s/it]
Traceback (most recent call last):
  File "/Users/shamba/Desktop/Paper 1 - HAR RL/NODATA_MarginMultistepCL/baselines/ts2vec.py", line 414, in <module>
    main(train_loader, valid_loader, valid_balanced_dataloader, seed)
  File "/Users/shamba/Desktop/Paper 1 - HAR RL/NODATA_MarginMultistepCL/baselines/ts2vec.py", line 220, in main
    train_loss.backward()
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/wandb/wandb_torch.py", line 271, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt